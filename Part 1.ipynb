{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(r\"E:\\Sheroo\\IBA - MBA\\4th Semester\\Text Analytics\\Final Project\\Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(data['headline'], data['is_sarcastic'], random_state=100,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Wall time: 93.8 ms\n",
      "Boost_Train Accuracy : 0.843\n",
      "Boost_Test Accuracy : 0.786\n",
      "Boost_Best Accuracy Through Grid Search : 0.782\n",
      "Boost_Best Parameters :  {'BST__learning_rate': 0.5, 'BST__max_depth': 5, 'vect__ngram_range': (1, 1)}\n",
      "--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->\n",
      "classification_report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.78      4537\n",
      "           1       0.74      0.85      0.79      4049\n",
      "\n",
      "    accuracy                           0.79      8586\n",
      "   macro avg       0.79      0.79      0.79      8586\n",
      "weighted avg       0.79      0.79      0.79      8586\n",
      "\n",
      "Best score: 0.782\n",
      "Best parameters set:\n",
      "\tBST__learning_rate: 0.5\n",
      "\tBST__max_depth: 5\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('vect', TfidfVectorizer()),\n",
    "                ('BST', GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, random_state=0)),\n",
    "               ])\n",
    "\n",
    "parameters = {\n",
    "    \"vect__ngram_range\": [(1, 1),(1,3),(1,5)],\n",
    "#    \"BST__n_estimators\": [50,100,200],\n",
    "    \"BST__max_depth\": [3,4,5],\n",
    "    \"BST__learning_rate\": [0.5,1.0]\n",
    "}\n",
    "\n",
    "grid_search_GB = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=3)\n",
    "\n",
    "grid_search_GB.fit(X_train, Y_train)\n",
    "\n",
    "%time GB_pred = grid_search_GB.predict(X_test)\n",
    "print('Boost_Train Accuracy : %.3f'%grid_search_GB.best_estimator_.score(X_train, Y_train))\n",
    "print('Boost_Test Accuracy : %.3f'%grid_search_GB.best_estimator_.score(X_test, Y_test))\n",
    "print('Boost_Best Accuracy Through Grid Search : %.3f'%grid_search_GB.best_score_)\n",
    "print('Boost_Best Parameters : ',grid_search_GB.best_params_)\n",
    "print(15*'--->--->')\n",
    "print('classification_report: \\n',  classification_report(Y_test, GB_pred))\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search_GB.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search_GB.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5; 1/2] START vect__ngram_range=(1, 1)....................................\n",
      "[CV 1/5; 1/2] END .....vect__ngram_range=(1, 1);, score=0.848 total time= 2.3min\n",
      "[CV 2/5; 1/2] START vect__ngram_range=(1, 1)....................................\n",
      "[CV 2/5; 1/2] END .....vect__ngram_range=(1, 1);, score=0.833 total time= 1.9min\n",
      "[CV 3/5; 1/2] START vect__ngram_range=(1, 1)....................................\n",
      "[CV 3/5; 1/2] END .....vect__ngram_range=(1, 1);, score=0.850 total time= 1.9min\n",
      "[CV 4/5; 1/2] START vect__ngram_range=(1, 1)....................................\n",
      "[CV 4/5; 1/2] END .....vect__ngram_range=(1, 1);, score=0.841 total time= 1.9min\n",
      "[CV 5/5; 1/2] START vect__ngram_range=(1, 1)....................................\n",
      "[CV 5/5; 1/2] END .....vect__ngram_range=(1, 1);, score=0.846 total time= 1.9min\n",
      "[CV 1/5; 2/2] START vect__ngram_range=(1, 2)....................................\n",
      "[CV 1/5; 2/2] END .....vect__ngram_range=(1, 2);, score=0.842 total time= 3.0min\n",
      "[CV 2/5; 2/2] START vect__ngram_range=(1, 2)....................................\n",
      "[CV 2/5; 2/2] END .....vect__ngram_range=(1, 2);, score=0.836 total time= 2.9min\n",
      "[CV 3/5; 2/2] START vect__ngram_range=(1, 2)....................................\n",
      "[CV 3/5; 2/2] END .....vect__ngram_range=(1, 2);, score=0.853 total time= 3.1min\n",
      "[CV 4/5; 2/2] START vect__ngram_range=(1, 2)....................................\n",
      "[CV 4/5; 2/2] END .....vect__ngram_range=(1, 2);, score=0.844 total time= 2.9min\n",
      "[CV 5/5; 2/2] START vect__ngram_range=(1, 2)....................................\n",
      "[CV 5/5; 2/2] END .....vect__ngram_range=(1, 2);, score=0.855 total time= 2.9min\n",
      "Wall time: 29.5 s\n",
      "SVC_Train Accuracy : 1.000\n",
      "SVC_Test Accuracy : 0.853\n",
      "SVC_Best Accuracy Through Grid Search : 0.846\n",
      "SVC_Best Parameters :  {'vect__ngram_range': (1, 2)}\n",
      "--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->--->\n",
      "classification_report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      4537\n",
      "           1       0.84      0.86      0.85      4049\n",
      "\n",
      "    accuracy                           0.85      8586\n",
      "   macro avg       0.85      0.85      0.85      8586\n",
      "weighted avg       0.85      0.85      0.85      8586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer()),\n",
    "                ('SVC', SVC(C=10, kernel='rbf', max_iter=-1,\n",
    "                                random_state=None ,cache_size=50,degree=1)),\n",
    "               ])\n",
    "\n",
    "parameters = {\n",
    "    \"vect__ngram_range\": [(1, 1),(1,2)]\n",
    "}\n",
    "\n",
    "grid_search_LSVC = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=10)\n",
    "grid_search_LSVC.fit(X_train, Y_train)\n",
    "\n",
    "%time LSVC_pred = grid_search_LSVC.predict(X_test)\n",
    "print('SVC_Train Accuracy : %.3f'%grid_search_LSVC.best_estimator_.score(X_train, Y_train))\n",
    "print('SVC_Test Accuracy : %.3f'%grid_search_LSVC.best_estimator_.score(X_test, Y_test))\n",
    "print('SVC_Best Accuracy Through Grid Search : %.3f'%grid_search_LSVC.best_score_)\n",
    "print('SVC_Best Parameters : ',grid_search_LSVC.best_params_)\n",
    "print(15*'--->--->')\n",
    "print('classification_report: \\n',  classification_report(Y_test, LSVC_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "627/627 - 7s - loss: 0.5162 - accuracy: 0.7234 - val_loss: 0.3838 - val_accuracy: 0.8259 - 7s/epoch - 12ms/step\n",
      "Epoch 2/30\n",
      "627/627 - 6s - loss: 0.2874 - accuracy: 0.8794 - val_loss: 0.3630 - val_accuracy: 0.8464 - 6s/epoch - 10ms/step\n",
      "Epoch 3/30\n",
      "627/627 - 6s - loss: 0.2073 - accuracy: 0.9207 - val_loss: 0.3767 - val_accuracy: 0.8449 - 6s/epoch - 10ms/step\n",
      "Epoch 4/30\n",
      "627/627 - 6s - loss: 0.1595 - accuracy: 0.9408 - val_loss: 0.4175 - val_accuracy: 0.8400 - 6s/epoch - 10ms/step\n",
      "Epoch 5/30\n",
      "627/627 - 6s - loss: 0.1245 - accuracy: 0.9565 - val_loss: 0.4741 - val_accuracy: 0.8362 - 6s/epoch - 10ms/step\n",
      "Epoch 6/30\n",
      "627/627 - 6s - loss: 0.0950 - accuracy: 0.9692 - val_loss: 0.5490 - val_accuracy: 0.8318 - 6s/epoch - 10ms/step\n",
      "Epoch 7/30\n",
      "627/627 - 6s - loss: 0.0713 - accuracy: 0.9770 - val_loss: 0.6074 - val_accuracy: 0.8293 - 6s/epoch - 10ms/step\n",
      "Epoch 8/30\n",
      "627/627 - 6s - loss: 0.0508 - accuracy: 0.9847 - val_loss: 0.7318 - val_accuracy: 0.8240 - 6s/epoch - 10ms/step\n",
      "Epoch 9/30\n",
      "627/627 - 6s - loss: 0.0349 - accuracy: 0.9899 - val_loss: 0.8020 - val_accuracy: 0.8239 - 6s/epoch - 10ms/step\n",
      "Epoch 10/30\n",
      "627/627 - 6s - loss: 0.0244 - accuracy: 0.9932 - val_loss: 0.9407 - val_accuracy: 0.8215 - 6s/epoch - 10ms/step\n",
      "Epoch 11/30\n",
      "627/627 - 6s - loss: 0.0164 - accuracy: 0.9958 - val_loss: 1.0043 - val_accuracy: 0.8185 - 6s/epoch - 10ms/step\n",
      "Epoch 12/30\n",
      "627/627 - 6s - loss: 0.0110 - accuracy: 0.9980 - val_loss: 1.1802 - val_accuracy: 0.8176 - 6s/epoch - 10ms/step\n",
      "Epoch 13/30\n",
      "627/627 - 6s - loss: 0.0075 - accuracy: 0.9980 - val_loss: 1.2643 - val_accuracy: 0.8180 - 6s/epoch - 10ms/step\n",
      "Epoch 14/30\n",
      "627/627 - 6s - loss: 0.0050 - accuracy: 0.9989 - val_loss: 1.2901 - val_accuracy: 0.8152 - 6s/epoch - 10ms/step\n",
      "Epoch 15/30\n",
      "627/627 - 6s - loss: 0.0039 - accuracy: 0.9993 - val_loss: 1.4623 - val_accuracy: 0.8156 - 6s/epoch - 10ms/step\n",
      "Epoch 16/30\n",
      "627/627 - 6s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 1.3345 - val_accuracy: 0.8142 - 6s/epoch - 9ms/step\n",
      "Epoch 17/30\n",
      "627/627 - 6s - loss: 0.0039 - accuracy: 0.9992 - val_loss: 1.4366 - val_accuracy: 0.8137 - 6s/epoch - 9ms/step\n",
      "Epoch 18/30\n",
      "627/627 - 6s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 1.6143 - val_accuracy: 0.8146 - 6s/epoch - 9ms/step\n",
      "Epoch 19/30\n",
      "627/627 - 6s - loss: 0.0060 - accuracy: 0.9981 - val_loss: 1.6371 - val_accuracy: 0.8148 - 6s/epoch - 9ms/step\n",
      "Epoch 20/30\n",
      "627/627 - 6s - loss: 0.0026 - accuracy: 0.9996 - val_loss: 1.6000 - val_accuracy: 0.8140 - 6s/epoch - 9ms/step\n",
      "Epoch 21/30\n",
      "627/627 - 6s - loss: 0.0020 - accuracy: 0.9995 - val_loss: 1.6550 - val_accuracy: 0.8134 - 6s/epoch - 9ms/step\n",
      "Epoch 22/30\n",
      "627/627 - 6s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.6143 - val_accuracy: 0.8135 - 6s/epoch - 9ms/step\n",
      "Epoch 23/30\n",
      "627/627 - 6s - loss: 0.0027 - accuracy: 0.9994 - val_loss: 1.7292 - val_accuracy: 0.8125 - 6s/epoch - 9ms/step\n",
      "Epoch 24/30\n",
      "627/627 - 6s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.7887 - val_accuracy: 0.8097 - 6s/epoch - 9ms/step\n",
      "Epoch 25/30\n",
      "627/627 - 6s - loss: 0.0013 - accuracy: 0.9997 - val_loss: 1.8323 - val_accuracy: 0.8111 - 6s/epoch - 9ms/step\n",
      "Epoch 26/30\n",
      "627/627 - 6s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.8038 - val_accuracy: 0.8106 - 6s/epoch - 9ms/step\n",
      "Epoch 27/30\n",
      "627/627 - 6s - loss: 0.0029 - accuracy: 0.9990 - val_loss: 1.8109 - val_accuracy: 0.8079 - 6s/epoch - 9ms/step\n",
      "Epoch 28/30\n",
      "627/627 - 6s - loss: 0.0031 - accuracy: 0.9991 - val_loss: 1.9019 - val_accuracy: 0.8093 - 6s/epoch - 10ms/step\n",
      "Epoch 29/30\n",
      "627/627 - 6s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 1.8836 - val_accuracy: 0.8077 - 6s/epoch - 9ms/step\n",
      "Epoch 30/30\n",
      "627/627 - 6s - loss: 0.0013 - accuracy: 0.9998 - val_loss: 1.8787 - val_accuracy: 0.8095 - 6s/epoch - 9ms/step\n"
     ]
    }
   ],
   "source": [
    "#CNN Model\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "training_size = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "training_padded = pad_sequences(training_sequences,maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(Y_train)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(Y_test)\n",
    "\n",
    "modelCNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "modelCNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "CNN = modelCNN.fit(training_padded, training_labels, epochs=30, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "627/627 - 75s - loss: 0.6038 - accuracy: 0.6027 - val_loss: 0.4369 - val_accuracy: 0.7738 - 75s/epoch - 120ms/step\n",
      "Epoch 2/10\n",
      "627/627 - 62s - loss: 0.3688 - accuracy: 0.8203 - val_loss: 0.3740 - val_accuracy: 0.8252 - 62s/epoch - 99ms/step\n",
      "Epoch 3/10\n",
      "627/627 - 63s - loss: 0.2861 - accuracy: 0.8765 - val_loss: 0.3525 - val_accuracy: 0.8427 - 63s/epoch - 101ms/step\n",
      "Epoch 4/10\n",
      "627/627 - 64s - loss: 0.2312 - accuracy: 0.9098 - val_loss: 0.3586 - val_accuracy: 0.8489 - 64s/epoch - 102ms/step\n",
      "Epoch 5/10\n",
      "627/627 - 63s - loss: 0.1923 - accuracy: 0.9286 - val_loss: 0.3780 - val_accuracy: 0.8440 - 63s/epoch - 100ms/step\n",
      "Epoch 6/10\n",
      "627/627 - 63s - loss: 0.1579 - accuracy: 0.9427 - val_loss: 0.4171 - val_accuracy: 0.8502 - 63s/epoch - 100ms/step\n",
      "Epoch 7/10\n",
      "627/627 - 63s - loss: 0.1362 - accuracy: 0.9515 - val_loss: 0.4555 - val_accuracy: 0.8444 - 63s/epoch - 101ms/step\n",
      "Epoch 8/10\n",
      "627/627 - 64s - loss: 0.1136 - accuracy: 0.9601 - val_loss: 0.4947 - val_accuracy: 0.8425 - 64s/epoch - 102ms/step\n",
      "Epoch 9/10\n",
      "627/627 - 63s - loss: 0.0964 - accuracy: 0.9680 - val_loss: 0.5765 - val_accuracy: 0.8399 - 63s/epoch - 100ms/step\n",
      "Epoch 10/10\n",
      "627/627 - 63s - loss: 0.0824 - accuracy: 0.9721 - val_loss: 0.6090 - val_accuracy: 0.8410 - 63s/epoch - 101ms/step\n"
     ]
    }
   ],
   "source": [
    "#Recurrent Nueral Network\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "training_size = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "training_padded = pad_sequences(training_sequences,maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(Y_train)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(Y_test)\n",
    "\n",
    "modelRNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "modelRNN.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])\n",
    "RNN = modelRNN.fit(training_padded, training_labels, epochs=10, validation_data=(testing_padded, testing_labels), verbose=2, validation_steps=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d28e2b66aef5d2c17aeeb5b592cdd1e982907e58d05755bc433fc9ad75ed81e2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
